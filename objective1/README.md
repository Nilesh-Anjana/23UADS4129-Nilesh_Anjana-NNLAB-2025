
## Perceptron Implementation for NAND & XOR Gates

1. Implements a single-layer perceptron with random weight initialization and bias.  
2. Uses a step activation function for binary classification.  
3. Trains using the perceptron learning algorithm, updating weights and bias based on prediction errors.  
4. Supports binary classification on NAND and XOR logic gates.  
5. Achieves 100% accuracy on NAND, as it is linearly separable.  
6. Fails to learn XOR, since it is not linearly separable and requires a multi-layer perceptron (MLP).  
7. Uses NumPy for efficient matrix operations and optimized computations.  
8. Demonstrates the limitations of single-layer perceptrons in solving complex patterns.  
