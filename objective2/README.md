# Multi-Layer Perceptron (MLP) Implementation for XOR Gate

1. Implements a Multi-Layer Perceptron (MLP) with one hidden layer.  
2. Uses random weight initialization and bias for learning.  
3. Employs a step activation function to produce binary outputs.  
4. Trains using perceptron weight update rules with a predefined learning rate.  
5. Backpropagation updates both input-to-hidden and hidden-to-output weights.  
6. Designed specifically to solve the XOR problem, which is not linearly separable.  
7. Uses NumPy for optimized matrix operations and efficient computation.  
8. Tracks training progress with loss and accuracy metrics printed every 1000 epochs.  
9. Demonstrates how multi-layer networks can solve non-linearly separable problems.  
10. After training, p
